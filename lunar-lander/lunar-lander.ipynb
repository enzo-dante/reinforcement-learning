{"cells":[{"cell_type":"markdown","metadata":{},"source":["# import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gym # env\n","from gym import wrappers\n","\n","import keras # model creation\n","\n","import numpy as np # handle matrix calculations\n","\n","import os\n","# keep logs to a minimum\n","os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5taRpTycmjhW"},"source":["# define memory\n","\n","the memory will house a stack of experiences that will be periodically sampled for experience replay learning\n","\n","we sample non-sequential memories to get a broad generalization of the environment\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"R0ghtxI9lAzK"},"outputs":[],"source":["import numpy as np\n","\n","\n","class Memory(\n","    object\n","):  # experience replay allows us to sample non-sequential memories that deters linear correlations\n","    def __init__(self, memory_size, state_size, action_space, one_hot_encoding=False):\n","        self.memory_size = memory_size # max num of batch of memories saved\n","        self.memory_counter = 0 # index of last memory saved\n","\n","        # categorize actions into binary values to differentiate into easy computations\n","        self.one_hot_encoding = (\n","            one_hot_encoding\n","        )\n","        # create a matrix of zeros with specified shape for states\n","        self.state_memory = np.zeros(\n","            (self.memory_size, state_size)\n","        )\n","\n","        # create a matrix of zeros with specified shape for new_states\n","        self.next_state_memory = np.zeros((self.memory_size, state_size))\n","\n","        # discrete = definitive set of integers\n","        # continuous = real numeric value, use float 32 format because calculations run faster\n","        action_type = (\n","            np.int8 if self.one_hot_encoding else np.float32\n","        )  # continuous actions = valid integer nums\n","\n","        self.action_memory = np.zeros(\n","            (self.memory_size, action_space), dtype=action_type\n","        )\n","\n","        # scalar because simple tracking without shape\n","        self.reward_memory = np.zeros(self.memory_size)\n","\n","        # if episode over, do NOT sample next state, reward = 0\n","        self.done_memory = np.zeros(\n","            self.memory_size, dtype=np.float32\n","        )\n","\n","    def remember(\n","        self, state, action, reward, next_state, done\n","    ):  # (state, action, reward, next_state, done)\n","\n","        # memory counter loops back to beginning to track & override stack within valid finite range\n","        index = self.memory_counter % self.memory_size  # stack\n","\n","        # add/override current state to state_memory at array position index\n","        self.state_memory[index] = state\n","        self.next_state_memory[index] = next_state\n","\n","        # store one-hot encoding if discrete = true\n","        # the one-hot encoding technique transforms nominal (order does not matter) categorical features & creates new binary columns for each observation\n","        # adding columns to your dataset of 1's and 0's\n","        # one-hot encoding is not always a good choice when there are to many categories\n","        if self.one_hot_encoding:\n","            actions = np.zeros(\n","                self.action_memory.shape[1]\n","            )\n","            actions[action] = 1.0\n","            self.action_memory[index] = actions\n","        else:\n","            self.action_memory[index] = action\n","\n","        self.reward_memory[index] = reward\n","        self.done_memory[index] = 1 - int(done) # episode over = 0; env returns True = 1\n","\n","        self.memory_counter += 1\n","\n","    # collect subset of memories to periodically update Q-values\n","    def sample(self, batch_size):\n","        # sample only valid memories (non-zero); array from 0 - (max_mem-1)\n","        max_memories = min(\n","            self.memory_counter, self.memory_size\n","        )\n","\n","        # randomize data selection to avoid overfitting / selecting same actions because of data concentration\n","        mini_batch = np.random.choice(max_memories, batch_size)\n","\n","        # subset of experiences\n","        states_mb = self.state_memory[mini_batch]\n","        next_states_mb = self.next_state_memory[mini_batch]\n","        actions_mb = self.action_memory[mini_batch]\n","        rewards_mb = self.reward_memory[mini_batch]\n","        done_mb = self.done_memory[mini_batch]\n","\n","        return states_mb, actions_mb, rewards_mb, next_states_mb, done_mb\n",""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pHQTcP5Z177h"},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# define the model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"OPLhUigC4qBB"},"outputs":[],"source":["class ANN(object):\n","    def __init__(self, action_space, learning_rate, state_size):\n","        self.action_space = action_space\n","        self.alpha = learning_rate\n","        self.state_size = state_size # (width, height)\n","        self.model = keras.Sequential()\n","\n","        # the sequence of the layers determine how the dataset is preprocessed\n","        # relu = rectifier function that allows for the account for non-linear effects: variables that influence each other but don't have a direct correlation\n","        self.model.add(\n","            keras.layers.Dense(units=256, input_shape=(self.state_size,), activation=\"relu\")\n","        )\n","        # dense layer = by the inputs to the outputs by establishing & refining weights (relationships)\n","        # input_shape w hanging comma implies batch\n","        self.model.add(\n","            keras.layers.Dense(units=256, input_shape=(self.state_size,), activation=\"relu\")\n","        )\n","        # final layer needs to match action space so that # of predictions match # of possible actions\n","        self.model.add(keras.layers.Dense(self.action_space, activation=\"softmax\"))\n","\n","        print(self.model.summary())\n","        # track gradient descent\n","        self.model.compile(optimizer=keras.optimizers.Adam(lr=self.alpha), loss=\"mse\")\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8lGqZyclSxmE"},"source":["# deep Q-learning issues\n","\n","using the same network to both choose the best action and evalue the quality of that action leads to learning instability\n","\n","the bellman max function in calculating the target value inherently bias towards short-term high rewards\n","\n","this can lead to the model getting stuck in a local minimum\n","\n","![](https://drive.google.com/uc?id=1P2oEWUdGZRt6SNpvjQif240nlAmPcu8l)\n","\n","**when we calculate the loss, no moving target**\n","\n","- It’s like if you were a cowboy (the Q estimation) and you want to catch the cow (the Q-target) you must get closer (reduce the error).\n","\n","- At each time step, you’re trying to approach the cow, which also moves at each time step (because you use the same parameters).\n","\n","![](https://drive.google.com/uc?id=1Jru3e5bvzmf43FW2ayo6q0X0qxzJT0T5)\n","\n","# Double Q-Learning with Dueling Architecture\n","\n","take the argmax of the outputs of the online network, but the Q-value for this action is evaluated from the target network (bellman equation)\n","\n","![](https://drive.google.com/uc?id=1QPApz4O9u6Tuez8k1lUuJMy2wkG2XpIF)\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"alDcfhNZJL3n"},"source":["# define agent"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"zTfBQ8ehD0O2"},"outputs":[],"source":["class Agent(object):\n","    def __init__(self, actions, state_size):\n","        self.actions = actions # number of actions\n","        self.action_space = [i for i in range(actions)] # list of defined actions in env\n","        self.batch_size = 64\n","        self.epsilon = 1.0 # probability of random actions taken\n","        self.epsilon_decay = 0.95 # rate at which random actions are overtaken by trained actions\n","        self.epsilon_min = 0.1\n","        self.learning_rate = 0.0003\n","        self.gamma = 0.9 # discount factor\n","        self.memory = Memory(1000000, 8, self.actions, True)\n","        self.name = \"DDQN.h5\"\n","        self.state_size = state_size\n","\n","        self.online_network = self.buildNetwork()  # takes actions and builds up experience\n","        self.target_network = self.buildNetwork()  # determines Q-value policy\n","\n","        self.update_target = 100  # update target_network weights after online_network has collected x amount of experiences\n","\n","    def buildNetwork(self):\n","        return ANN(self.actions, self.learning_rate, self.state_size)\n","\n","    def chooseAction(self, state):\n","\n","        state = state[np.newaxis, :] # allow for batch training and single memory\n","        random_num = np.random.random() # generate random number\n","\n","        # epsilon-greedy action selection policy\n","        # if randomly generated number is less than epsilon, than take random action\n","        if random_num < self.epsilon:\n","            # choose random action\n","            action = np.random.choice(self.action_space)\n","        else:\n","            # else take trained actions predicted by neural network (Q-value of current state)\n","            actions = self.online_network.model.predict(\n","                state\n","            )\n","            # identify best action in set of possible actions per state\n","            action = np.argmax(actions)\n","\n","        return action\n","\n","    # copy online_network weights and replace for target_network weights\n","    def updateTargetWeights(self):\n","        self.target_network.model.set_weights(self.online_network.model.get_weights())\n","\n","    def save(self):\n","        self.online_network.model.save(\"/models/\" + self.name)\n","\n","    def loadModel(self, file):\n","        self.online_network = tf.keras.models.load_model(\"/models/\" + file)\n","\n","        # make sure target_network and online_network always start with equal weights\n","        if self.epsilon == self.epsilon_min:\n","            self.updateTargetWeights()\n","\n","    def learn(self):\n","        # pre-train process is waiting for the memory to fill up with experiences before starting training process\n","        if self.memory.memory_counter > self.batch_size:\n","            state, action, reward, next_state, done = self.memory.sample(\n","                self.batch_size\n","            )\n","            # convert actions back to regular encoding indices from one-hot encoding\n","            action_values = np.array(\n","                self.action_space, dtype=np.int8\n","            )\n","            action_labels = np.dot(action, action_values)\n","\n","            # double deep Q-learning\n","\n","            # best action in the next state\n","            Qs_next_online = self.online_network.model.predict(next_state)\n","            Qs_next_target = self.target_network.model.predict(next_state)\n","\n","            # actual Q-value of actions taken from the current state\n","            Qs_actual = self.online_network.model.predict(state)\n","\n","            # get max Q-values in next state\n","            Qs_target_next_state = np.argmax(Qs_next_online, axis=1)\n","\n","            # initialize a Q-value matrix to calculate the loss in gradient descent\n","            Qs_target = Qs_actual\n","\n","            # use done flag to identify if terminal state has been reached\n","            batch_index = np.arange(self.batch_size, dtype=np.int32)\n","\n","            # use the bellman equation to calculate Q-target values[given_states, actions]\n","            # Q(s,a) = r + y * Q(s', a')\n","            Qs_target[batch_index, action_labels] = (\n","                reward\n","                + self.gamma\n","                * Qs_next_target[batch_index, Qs_target_next_state.astype(int)]\n","                * done\n","            )  # done ensures future states do not get sampled after terminal state\n","\n","            # the stabalized Q-value target prediction from the target_network based on the weights of the online network\n","            # use the fit() to calculate the loss with mse error: 1/2*(actual-predicted)^2\n","\n","            # process:\n","                # forward-propogate states through online cnn\n","                # take online prediction and compare it to Q-value target\n","                # comparison = loss\n","                # backpropogate loss to adjust weights\n","            _ = self.online_network.model.fit(\n","                state, Qs_target, verbose=0\n","            ) # verbose removes terminal info printing\n","\n","            # after weight adjustments, reduce probability of random actions taken if greater than epsilon_min\n","            self.epsilon = (\n","                self.epsilon * self.epsilon_decay\n","                if self.epsilon > self.epsilon_min\n","                else self.epsilon_min\n","            )\n","\n","            # update target_network weights if online_network has collected enough experiences\n","            if self.memory.memory_counter % self.update_target == 0:\n","                self.updateTargetWeights()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SRzhNeaLnPDs"},"source":["# setup main playthrough"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"rM6P6twJvWoZ"},"outputs":[],"source":["env = gym.make(\"LunarLander-v2\")\n","\n","agent = Agent(actions=4, state_size=8)\n","\n","total_episodes = 500\n","# agent.loadModel()\n","scores = []  # track agent progress\n","policy = []  # track epsilon greedy rate\n","\n","env = wrappers.Monitor(\n","    env, \"gameplay\", video_callable=lambda episode_id: True, force=True\n",")\n","\n","# training\n","for episode in range(total_episodes):\n","    done = False\n","    score = 0\n","    state = env.reset()  # start episode with initial state observation\n","    while not done:\n","        action = agent.chooseAction(state)\n","        next_state, reward, done, info = env.step(action)  # info = general env data\n","\n","        score += reward\n","\n","        agent.memory.remember(\n","            state, action, reward, next_state, done\n","        )  # store experience to memory for exp replay\n","        state = next_state  # transition to next state\n","        agent.learn()\n","\n","    policy.append(agent.epsilon)\n","    scores.append(score)\n","\n","    avg_score = np.mean(scores[max(0, episode - 100) : (episode + 1)])\n","    print(\n","        \"episode: \", episode, \"score: %.2f\" % score, \"avg score: %.2f\" % avg_score\n","    )\n","\n","    if episode % 10 == 0 and episode > 0:\n","        agent.save()\n","\n"]}],"metadata":{"colab":{"name":"lunar-lander.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1thT3iA7dX05Zef7bH8EU_VqgkQYEXUAt","authorship_tag":"ABX9TyMMYMhhI9b6bvbLT7oK3FnO"},"kernelspec":{"name":"python361064bitgymtf2condaeb0519c0fe954f9d9b0d455609bcf17b","display_name":"Python 3.6.10 64-bit ('gym-tf2': conda)"}},"nbformat":4,"nbformat_minor":0}